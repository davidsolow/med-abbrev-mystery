{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidsolow/med-abbrev-mystery/blob/ms-bert/MS_BERT_abbreviation_embedding_model_batching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1R447BZlh9q"
      },
      "source": [
        "# Import Statements and Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTQnC7Yj-PnU",
        "outputId": "4fb2969f-7d02-4ce8-9692-4888ba1bad51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "wBmVGnZ80kYd"
      },
      "outputs": [],
      "source": [
        "#Import statements\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import re\n",
        "import textwrap\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from transformers import BertTokenizer, TFBertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "12v3YJz1-0UH",
        "outputId": "d5e86afc-d35e-4680-dd8f-1550d994cd8a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'transformers' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-de6040bb8a15>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'transformers' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPO1c7E7dsej",
        "outputId": "e83cd28d-f0a5-4c10-ce19-436c6c5dedb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "#from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "access_token = \"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\"\n",
        "model_name = \"NLP4H/ms_bert\"\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(model_name, token = access_token)\n",
        "bert_model = TFBertModel.from_pretrained(model_name, token = access_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ3M1_qjfK9K",
        "outputId": "da216052-dc22-410b-8dc5-e7b35b1773ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "UK_ZnV7KfUso"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"drive/MyDrive/266Project/train-3.csv\")\n",
        "#test = pd.read_csv(\"drive/MyDrive/266Project/test.csv\")\n",
        "validation = pd.read_csv(\"drive/MyDrive/266Project/validation.csv\")[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNTNQqgSlG8A",
        "outputId": "725b7e56-becf-41ca-9464-25745bebb35a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000000\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "print(len(train))\n",
        "#print(len(test))\n",
        "print(len(validation))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_validation.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wFise3Xuj5VQ",
        "outputId": "dbac2b57-1dfb-4870-a374-1eea4fd5fda6"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  abstract_id                                               text  \\\n",
              "0           0      5017844  different electrocardiographic changes have be...   \n",
              "1           1      8923525  there are limited data regarding cdx expressio...   \n",
              "2           2      4675370  many healthcare facilities and other notforpro...   \n",
              "3           3      1402383  we have tested the functional compatibility be...   \n",
              "4           4      8577593  we report the case of an monthold infant who w...   \n",
              "\n",
              "   location                                label abbreviation  \n",
              "0        25    transluminal coronary angioplasty          tca  \n",
              "1        14            colorectal adenocarcinoma          crc  \n",
              "2       109                         notforprofit          nfp  \n",
              "3        10    human immunodeficiency virus type        hiv-1  \n",
              "4        15  extracorporeal membrane oxygenation         ecmo  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f23e2268-3a77-4f84-9d19-6b6ff107d77d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>abstract_id</th>\n",
              "      <th>text</th>\n",
              "      <th>location</th>\n",
              "      <th>label</th>\n",
              "      <th>abbreviation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>5017844</td>\n",
              "      <td>different electrocardiographic changes have be...</td>\n",
              "      <td>25</td>\n",
              "      <td>transluminal coronary angioplasty</td>\n",
              "      <td>tca</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8923525</td>\n",
              "      <td>there are limited data regarding cdx expressio...</td>\n",
              "      <td>14</td>\n",
              "      <td>colorectal adenocarcinoma</td>\n",
              "      <td>crc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>4675370</td>\n",
              "      <td>many healthcare facilities and other notforpro...</td>\n",
              "      <td>109</td>\n",
              "      <td>notforprofit</td>\n",
              "      <td>nfp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1402383</td>\n",
              "      <td>we have tested the functional compatibility be...</td>\n",
              "      <td>10</td>\n",
              "      <td>human immunodeficiency virus type</td>\n",
              "      <td>hiv-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>8577593</td>\n",
              "      <td>we report the case of an monthold infant who w...</td>\n",
              "      <td>15</td>\n",
              "      <td>extracorporeal membrane oxygenation</td>\n",
              "      <td>ecmo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f23e2268-3a77-4f84-9d19-6b6ff107d77d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f23e2268-3a77-4f84-9d19-6b6ff107d77d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f23e2268-3a77-4f84-9d19-6b6ff107d77d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2baafef1-77ff-43a5-ae50-1ccaea900c12\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2baafef1-77ff-43a5-ae50-1ccaea900c12')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2baafef1-77ff-43a5-ae50-1ccaea900c12 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "clean_validation",
              "summary": "{\n  \"name\": \"clean_validation\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          8,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4541946,\n        \"min\": 1402383,\n        \"max\": 14676253,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          12275488,\n          8923525,\n          7251799\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"giant cmn are rare congenital disfiguring benign neoplasms with a risk of transformation to malignant melanoma they often present with various extracutaneous features here we describe a case of giant melanocytic nevus with developmental dysplasia of bl hip a novel association\",\n          \"there are limited data regarding cdx expression in rectal carcinoma the ckck immunoprofile of crc has been described in studies which have mostly lumped tc and rectal pt together in this study we investigated the diagnostic utility of immunohistochemical stains for ck ck and cdx in a series of rectal adenocarcinoma fiftyfive specimens of rectal adenocarcinomas were retrieved and immunostained for ck dakom ck novocastra ncllck and cdx novocastra nclcdx thirty cases of pancreatic adenocarcinoma and cc were also studied as a comparison group ck was expressed in and ck in cases of rectal adenocarcinoma the ckck immunophenotype was identified in ckck in and ckck in rectal adenocarcinoma cdx showed moderatestrong positivity in all cases and was not related to rt differentiation benign rectal mucosa was available in cases and showed the following results ckck in ckck in and ckck in cases in pancreatic adenocarcinomas and cholangiocarcinomas were ckck and were ckck cdx was positive in only of these cases all were pancreatic adenocarcinomas in conclusion ck can be expressed in rectal adenocarcinoma and should not be used as the sole basis for excluding a rectal primary cdx is a cs marker for rectal origin of adenocarcinoma it can be helpful in cases with metastatic rectal carcinoma especially those with ckck or ckck immunophenotype in this study cdx expression was not influenced by the grade differentiation of rectal adenocarcinoma\",\n          \"the betaisopropylmalate dehydrogenase leu gene from a homothallic wildtype yeast saccharomyces exiguus ypl was analyzed to estimate the phylogenetic position of this cs in yeasts the betaisopropylmalate dehydrogenase gene of ypl was first isolated as a clone complementing the leu mutation of saccharomyces cerevisiae and then confirmed to complement the haploid leu mutant derived from strain ypl through genetic transformation the nucleotide cs of the cloned dna revealed an open reading frame orf encoding the betaisopropylmalate dehydrogenase composed of amino acids the betaisopropylmalate dehydrogenase coding sequence from the ypl cs displayed similarity to that of s cerevisiae candidates for a uas and a tatabox in the upstream region and for a polya att in the downstream region were found a phylogenetic tree constructed from the nucleotide sequences of the betaisopropylmalate dehydrogenase coding regions revealed that ypl is located between s cerevisiae and the kluyveromyces yeasts the leu gene cloned from ypl will serve as an effective genetic marker for constructing the transformation system in s exiguus ypl\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"location\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 42,\n        \"min\": 0,\n        \"max\": 112,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1,\n          14,\n          112\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"congenital melanocytic nevi\",\n          \"colorectal adenocarcinoma\",\n          \"attachment site\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abbreviation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"cmn\",\n          \"crc\",\n          \"att\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtZZX18klcLF"
      },
      "source": [
        "# Exploratory Data Analysis and Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "q4_o88VP1ady"
      },
      "outputs": [],
      "source": [
        "#cleaning locations and labels\n",
        "max_length = 200\n",
        "max_location = max_length - 3 # minus [CLS] and [SEP] tokens added and index offset\n",
        "\n",
        "def clean_location(location):\n",
        "  \"\"\"Takes a number in brackets as input and returns the number as an int\"\"\"\n",
        "  return int(str(location).strip(\"[]\"))\n",
        "\n",
        "\n",
        "def clean_label(label):\n",
        "  \"\"\"Takes a label in brackets and quotes as input and returns the label as a string\"\"\"\n",
        "  return label.strip(\"[]'\")\n",
        "\n",
        "  [dataset['location'] <= max_location]\n",
        "\n",
        "\n",
        "def add_abbreviation_col(dataset):\n",
        "    \"\"\"Adds an abbreviation column to the dataset from the specified location in the text\"\"\"\n",
        "    dataset['abbreviation'] = dataset.apply(lambda row: row['text'].lower().split()[row['location']], axis=1)\n",
        "    return dataset\n",
        "\n",
        "def make_lower(text):\n",
        "  return text.lower()\n",
        "\n",
        "\n",
        "def clean_dataset(dataset):\n",
        "    dataset['text'] = dataset['text'].apply(make_lower)\n",
        "    dataset['location'] = dataset['location'].apply(clean_location)\n",
        "    dataset['label'] = dataset['label'].apply(clean_label)\n",
        "    dataset = dataset.loc[dataset['location'] <= max_location].copy()\n",
        "    add_abbreviation_col(dataset)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train = clean_dataset(train)\n",
        "#test = clean_dataset(test)\n",
        "#validation = clean_dataset(validation)\n",
        "\n",
        "#train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "815eqbxSTLOm"
      },
      "outputs": [],
      "source": [
        "#Label dict\n",
        "unique_labels = train['label'].unique()\n",
        "def make_label_map():\n",
        "  label_map = {}\n",
        "  for i in range(len(unique_labels)):\n",
        "    label_map[unique_labels[i]] = i\n",
        "  return label_map\n",
        "\n",
        "label_dict = make_label_map()\n",
        "#label_strs = list(label_dict.keys())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "7p847lRcDJOT"
      },
      "outputs": [],
      "source": [
        "#NUM_ABBREVIATIONS = 100\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "#abbreviation_subset = random.sample(list(train.abbreviation.unique()), NUM_ABBREVIATIONS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "_E8n7OC4RdV4"
      },
      "outputs": [],
      "source": [
        "del train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tyS6qJHewbX"
      },
      "source": [
        "# BERT Base Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "I-wUyVEMgq5Q"
      },
      "outputs": [],
      "source": [
        "# tokenize the subset dataset, truncate at `max_length`,\n",
        "# and pad with 0's when less than `max_length` and return a tf Tensor\n",
        "def tokenize(text):\n",
        "    return bert_tokenizer(\n",
        "        list(text),\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf',\n",
        "    )\n",
        "\n",
        "# train_encodings = tokenize(train_subset.text)\n",
        "# valid_encodings = tokenize(validation_subset.text)\n",
        "# #test_encodings = tokenize(test_subset.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "rUAqhgJsxzQg"
      },
      "outputs": [],
      "source": [
        "def get_abbrev_token_positions(text, abbrev):\n",
        "    \"\"\"\n",
        "    Takes text and abbreviation and finds the start and end index of the\n",
        "    tokenized representation of that abbreviation in the text\n",
        "    \"\"\"\n",
        "    tokenized_text = bert_tokenizer.tokenize(text)\n",
        "    tokenized_abbrev = bert_tokenizer.tokenize(abbrev)\n",
        "    token_ids_text = bert_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    token_ids_abbrev = bert_tokenizer.convert_tokens_to_ids(tokenized_abbrev)\n",
        "    start = -1\n",
        "    for i in range(len(token_ids_text) - len(token_ids_abbrev) + 1):\n",
        "        if token_ids_text[i:i+len(token_ids_abbrev)] == token_ids_abbrev:\n",
        "            start = i\n",
        "            break\n",
        "\n",
        "    if start == -1:\n",
        "        raise ValueError(f\"Abbreviation '{abbrev}' not found in text '{text}'\")\n",
        "\n",
        "    end = start + len(token_ids_abbrev) - 1\n",
        "    return start, end\n",
        "\n",
        "\n",
        "def extract_abbrev_positions_from_dataset(texts, abbreviations):\n",
        "    \"\"\"Extracts all the start and end position of each abbreviation in a dataset\"\"\"\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(texts)):\n",
        "        start, end = get_abbrev_token_positions(texts[i], abbreviations[i])\n",
        "        # add 1 to account for CLS token at start\n",
        "        # TODO: verify that this is actually needed and CLS token should be\n",
        "        #   accounted for like this\n",
        "        start_positions.append(start + 1)\n",
        "        end_positions.append(end + 1)\n",
        "\n",
        "    return start_positions, end_positions\n",
        "\n",
        "\n",
        "#train_start_positions, train_end_positions = extract_abbrev_positions_from_dataset(train_subset)\n",
        "#v#alid_start_positions, valid_end_positions = extract_abbrev_positions_from_dataset(validation_subset)\n",
        "#test_start_positions, test_end_positions = extract_abbrev_positions_from_dataset(test_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "Sr4QZzdrSd0z"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(df, text, label, tokenizer):\n",
        "    # With BERT tokenizer's batch_encode_plus, sentence pairs are\n",
        "    # encoded together and separated by [SEP] token.\n",
        "    #start_positions, end_positions = extract_abbrev_positions_from_dataset(df)\n",
        "    encoded = tokenize(text)\n",
        "    #print(encoded)\n",
        "\n",
        "    # Extract encoded features and labels, add to corresponding lists\n",
        "    input_ids = np.array(encoded.input_ids, dtype=\"int32\")\n",
        "    attention_masks = np.array(encoded.attention_mask, dtype=\"int32\")\n",
        "    token_type_ids = np.array(encoded.token_type_ids, dtype=\"int32\")\n",
        "    #token_ids_text = [bert_tokenizer.convert_tokens_to_ids(tokens) for tokens in encoded]\n",
        "    #print(token_ids_text)\n",
        "    start_positions, end_positions = extract_abbrev_positions_from_dataset(text, df.abbreviation.tolist())\n",
        "\n",
        "    # Convert string labels into numbered categories\n",
        "    labels = np.array([label_dict[i]\n",
        "                       for i in label])\n",
        "\n",
        "    return [input_ids, attention_masks, token_type_ids, np.array(start_positions), np.array(end_positions)], labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "kF5zEPsBbv4P"
      },
      "outputs": [],
      "source": [
        "clean_validation = clean_dataset(validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyxhX5s-ovdG"
      },
      "outputs": [],
      "source": [
        "validation = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "WDhXXGkYosB6"
      },
      "outputs": [],
      "source": [
        "max_length = 200\n",
        "validation_data = preprocess_data(clean_validation, clean_validation['text'].values.astype(str).tolist(), clean_validation['label'].values, bert_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "xjRdRtYbbn7s"
      },
      "outputs": [],
      "source": [
        "class SNLIDataGeneratorFromFile(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 n_examples,\n",
        "                 data_filename,\n",
        "                 max_length=200,\n",
        "                 batch_size=32,\n",
        "                 shuffle=True):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.n_examples = n_examples\n",
        "        self.data_filename = data_filename\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
        "        self.row_order = np.arange(1, self.n_examples + 1, dtype=int)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of batches in the full dataset\n",
        "        return self.n_examples // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_start = idx * self.batch_size\n",
        "        batch_end = (idx + 1) * self.batch_size\n",
        "\n",
        "        # Indices to skip are the ones in the shuffled row_order before and\n",
        "        # after the chunk we'll use for this batch\n",
        "        batch_idx_skip = np.concatenate((self.row_order[:batch_start], self.row_order[batch_end:])).astype(int)\n",
        "        batch_idx_skip = batch_idx_skip.tolist()  # Ensure this is a list of integers\n",
        "\n",
        "        # print(f\"Batch start: {batch_start}, Batch end: {batch_end}\")\n",
        "        # print(f\"Batch indices to skip: {batch_idx_skip[:10]}\")  # Print the first 10 indices for brevity\n",
        "\n",
        "        # Read CSV file with debugging statements\n",
        "        try:\n",
        "            # df = pd.read_csv(self.data_filename)\n",
        "            # print(f\"Dataframe shape without skiprows: {df.shape}\")\n",
        "            # print(f\"Dataframe columns without skiprows: {df.columns}\")\n",
        "            df = pd.read_csv(self.data_filename, skiprows=batch_idx_skip)\n",
        "            # print(f\"Dataframe shape: {df.shape}\")\n",
        "            # print(f\"Dataframe columns: {df.columns}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading CSV: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Check if specific columns are present\n",
        "        required_columns = ['text', 'label']\n",
        "        for col in required_columns:\n",
        "            if col not in df.columns:\n",
        "                raise KeyError(f\"Missing required column: {col}\")\n",
        "\n",
        "        df = clean_dataset(df)\n",
        "\n",
        "        # abbrev_positions = extract_abbrev_positions_from_dataset(df, df.abbreviation.tolist())\n",
        "        # start_abbrev_token_positions = abbrev_positions[0]\n",
        "        # end_abbrev_token_positions = abbrev_positions[1]\n",
        "\n",
        "        text_input = df['text'].values.astype(str).tolist()\n",
        "        labels = df['label'].values\n",
        "\n",
        "        batch_data = preprocess_data(df,\n",
        "                                     text_input,\n",
        "                                     labels,\n",
        "                                     self.tokenizer)\n",
        "\n",
        "        return batch_data\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.row_order = np.random.permutation(self.row_order).astype(int).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "RtWMrLi4tIun"
      },
      "outputs": [],
      "source": [
        "class ExtractAbbreviationHiddenStates(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom layer that extracts abbreviation embeddings from BERT\n",
        "    hidden layer state and position inputs\n",
        "    \"\"\"\n",
        "    def call(self, inputs):\n",
        "        last_hidden_state, start_abbrev_token_positions, end_abbrev_token_positions = inputs\n",
        "\n",
        "        batch_size = tf.shape(last_hidden_state)[0]\n",
        "        max_length = tf.shape(last_hidden_state)[1]\n",
        "        hidden_size = tf.shape(last_hidden_state)[2]\n",
        "\n",
        "        span_hidden_states = tf.TensorArray(tf.float32, size=batch_size)\n",
        "\n",
        "        for i in tf.range(batch_size):\n",
        "            start_pos = start_abbrev_token_positions[i, 0]\n",
        "            end_pos = end_abbrev_token_positions[i, 0]\n",
        "\n",
        "            # TODO: training fails without this... when do we expect start or end position to be\n",
        "            #   greater than 199?\n",
        "            start_pos = tf.clip_by_value(start_pos, 0, max_length - 1)\n",
        "            end_pos = tf.clip_by_value(end_pos, 0, max_length - 1)\n",
        "\n",
        "            span_hidden_state = last_hidden_state[i, start_pos:end_pos + 1, :]\n",
        "            span_length = end_pos - start_pos + 1\n",
        "\n",
        "            # pad to the maximum length\n",
        "            padded_span_hidden_state = tf.pad(span_hidden_state, [[0, max_length - span_length], [0, 0]])\n",
        "            span_hidden_states = span_hidden_states.write(i, padded_span_hidden_state)\n",
        "\n",
        "        return span_hidden_states.stack()\n",
        "\n",
        "\n",
        "def create_bert_multiclass_model(name = model_name,\n",
        "                                 num_classes = len(label_dict),\n",
        "                                 learning_rate=0.00005):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with BERT. Use the pooled abbreviation\n",
        "    token embeddings for classification purposes.\n",
        "    \"\"\"\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask_layer')\n",
        "    start_abbrev_token_positions = tf.keras.layers.Input(shape=(1,), dtype=tf.int32, name='start_abbreviation_token_positions_layer')\n",
        "    end_abbrev_token_positions = tf.keras.layers.Input(shape=(1,), dtype=tf.int32, name='end_abbreviation_token_positions_layer')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}\n",
        "\n",
        "    bert_model = TFBertModel.from_pretrained(name, from_pt=True)\n",
        "    bert_model.trainable = True\n",
        "\n",
        "    bert_out = bert_model(bert_inputs)\n",
        "\n",
        "    last_hidden_state = bert_out.last_hidden_state\n",
        "\n",
        "    span_hidden_states = ExtractAbbreviationHiddenStates()([last_hidden_state, start_abbrev_token_positions, end_abbrev_token_positions])\n",
        "\n",
        "    pooled_output = tf.reduce_mean(span_hidden_states, axis=1)\n",
        "\n",
        "    classification = tf.keras.layers.Dense(num_classes, activation='softmax', name='classification_layer')(pooled_output)\n",
        "\n",
        "    classification_model = tf.keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask, start_abbrev_token_positions, end_abbrev_token_positions],\n",
        "        outputs=[classification],\n",
        "    )\n",
        "\n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                                 metrics='accuracy')\n",
        "\n",
        "    return classification_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpVEBq88bSwY",
        "outputId": "7b71ed34-10d1-45a1-cca2-dc99da194354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " attention_mask_layer (Inpu  [(None, 200)]                0         []                            \n",
            " tLayer)                                                                                          \n",
            "                                                                                                  \n",
            " input_ids_layer (InputLaye  [(None, 200)]                0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " token_type_ids_layer (Inpu  [(None, 200)]                0         []                            \n",
            " tLayer)                                                                                          \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1094822   ['attention_mask_layer[0][0]',\n",
            " )                           ngAndCrossAttentions(last_   40         'input_ids_layer[0][0]',     \n",
            "                             hidden_state=(None, 200, 7              'token_type_ids_layer[0][0]']\n",
            "                             68),                                                                 \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " start_abbreviation_token_p  [(None, 1)]                  0         []                            \n",
            " ositions_layer (InputLayer                                                                       \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " end_abbreviation_token_pos  [(None, 1)]                  0         []                            \n",
            " itions_layer (InputLayer)                                                                        \n",
            "                                                                                                  \n",
            " extract_abbreviation_hidde  (None, None, 768)            0         ['tf_bert_model[0][0]',       \n",
            " n_states (ExtractAbbreviat                                          'start_abbreviation_token_pos\n",
            " ionHiddenStates)                                                   itions_layer[0][0]',          \n",
            "                                                                     'end_abbreviation_token_posit\n",
            "                                                                    ions_layer[0][0]']            \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpL  (None, 768)                  0         ['extract_abbreviation_hidden_\n",
            " ambda)                                                             states[0][0]']                \n",
            "                                                                                                  \n",
            " classification_layer (Dens  (None, 22555)                1734479   ['tf.math.reduce_mean[0][0]'] \n",
            " e)                                                       5                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 126827035 (483.81 MB)\n",
            "Trainable params: 126827035 (483.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_bert_multiclass_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "aOdIjn0ma9Ds"
      },
      "outputs": [],
      "source": [
        "train_data_generator = SNLIDataGeneratorFromFile(\n",
        "    tokenizer=bert_tokenizer,\n",
        "    n_examples=3000000,\n",
        "    data_filename='drive/MyDrive/266Project/train-3.csv'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "qqILsxD8bcY_"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = 'drive/MyDrive/266Project/ms_bert_checkpoints'\n",
        "checkpoint_filepath = checkpoint_dir + 'weights.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okLDsrW0W_Wi",
        "outputId": "6b1f0c68-4a9d-44a2-fb1f-79fa10a59056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch start: 0, Batch end: 32\n",
            "Batch indices to skip: [1256465, 905574, 2968785, 140855, 1407339, 2012838, 2543188, 2441148, 1745753, 1802768]\n",
            "Dataframe shape: (32, 5)\n",
            "Dataframe columns: Index(['Unnamed: 0', 'abstract_id', 'text', 'location', 'label'], dtype='object')\n",
            "([array([[  101,  4634,  2003, ...,  1052,  1998,   102],\n",
            "       [  101,  3729,  2019, ...,  2007,  5250,   102],\n",
            "       [  101,  7388,  6187, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [  101,  2048,  4937, ...,     0,     0,     0],\n",
            "       [  101,  8824,  4311, ..., 18856,  5300,   102],\n",
            "       [  101,  2195,  6887, ..., 19126,  8153,   102]], dtype=int32), array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32), array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32), array([111,  12,  24,  63,  25,  19, 113,  15,  73,  49,  22, 157,   9,\n",
            "       200, 237,  12, 259,  75,  29,  39, 110,  81,  44, 199,   6, 140,\n",
            "         1,  61, 124,  65]), array([112,  13,  25,  63,  26,  21, 114,  15,  73,  50,  22, 160,  11,\n",
            "       201, 238,  13, 260,  76,  30,  40, 110,  81,  44, 200,   9, 141,\n",
            "         1,  62, 124,  66])], array([14434, 14948, 11348, 10358, 12006,   725,  8933,  1161, 16394,\n",
            "       15627, 14204,  4757,  1146,  7678, 22388,  2693, 17075,  3749,\n",
            "       14927,  8265,  3234,   577,   377, 10113,  3549,  8485,  6113,\n",
            "       12700,  4650, 13172]))\n"
          ]
        }
      ],
      "source": [
        "print(next(iter(train_data_generator)))\n",
        "#print(next(iter(validation_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "bP5sBA3ebpmn",
        "outputId": "2d6b29a4-c54e-4c12-f671-209e96830348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   19/93750 [..............................] - ETA: 506:22:11 - loss: 10.0237 - accuracy: 0.0017"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-165-bcf192f315ad>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(train_data_generator, validation_data=(validation_data), epochs=3,\n\u001b[0m\u001b[1;32m      2\u001b[0m           callbacks=[model_checkpoint_callback])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1802\u001b[0m                         ):\n\u001b[1;32m   1803\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.fit(train_data_generator, validation_data=(validation_data), epochs=3,\n",
        "          callbacks=[model_checkpoint_callback])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}