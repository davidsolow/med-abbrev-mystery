{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statements\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "import gc\n",
    "\n",
    "max_length = 200\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_checkpoint = 'bert-base-cased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_checkpoint)\n",
    "offset = bert_tokenizer.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/medal_smaller_train.csv')\n",
    "val_df = pd.read_csv('../data/medal_smaller_validation.csv')\n",
    "test_df = pd.read_csv('../data/medal_smaller_validation.csv')\n",
    "\n",
    "samples = train_df.label.value_counts().sort_values(ascending=False)[:6500].index # Using the top 6500 occurring acronyms\n",
    "print(samples[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.loc[train_df['label'].isin(samples)]\n",
    "val_df = val_df.loc[val_df['label'].isin(samples)]\n",
    "test_df = test_df.loc[test_df['label'].isin(samples)]\n",
    "\n",
    "label_names = (list(train_df.label) +\n",
    "               list(val_df.label))\n",
    "label_names = sorted(list(set(label_names)))\n",
    "\n",
    "label_dict = {value: index + offset for index, value in enumerate(label_names)}\n",
    "reverse_label_dict = {value: key for key, value in label_dict.items()}\n",
    "print(f\"Size: {len(label_names)}\", label_names)\n",
    "print(f\"Dict Item 1: {list(label_dict.items())[0]}\")\n",
    "print(f\"Reverse Dict Item 1: {list(reverse_label_dict.items())[0]}\")\n",
    "\n",
    "print(\"Train dataset length:\", len(train_df))\n",
    "print(\"Validation dataset length:\", len(val_df))\n",
    "\n",
    "with open('../vocabulary/dictionary.json', 'w') as json_file:\n",
    "    json.dump(label_dict, json_file)\n",
    "\n",
    "with open('../vocabulary/reverse_dictionary.json', 'w') as json_file:\n",
    "    json.dump(reverse_label_dict, json_file)\n",
    "\n",
    "with open('../vocabulary/label_names.json', 'w') as json_file:\n",
    "    json.dump(label_names, json_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset, tokenizer=bert_tokenizer, max_len=max_length, label_dict=label_dict):\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    attention_masks = []\n",
    "    label_ids = []\n",
    "\n",
    "    for text, loc, abbreviation, label in zip(dataset['text'], dataset['location'], dataset['abbreviation'], dataset['label']):\n",
    "        pre_tokens = tokenizer.tokenize(' '.join(text.split()[:loc]))\n",
    "        adjusted_loc_start = len(pre_tokens) + 1\n",
    "        adjusted_loc_end = adjusted_loc_start + len(tokenizer.tokenize(abbreviation))\n",
    "        encoded_input = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        \n",
    "        if adjusted_loc_end < max_length:\n",
    "            input_ids.append(encoded_input['input_ids'])\n",
    "            token_type_ids.append(encoded_input['token_type_ids'])\n",
    "            start_positions.append(adjusted_loc_start)\n",
    "            end_positions.append(adjusted_loc_end)\n",
    "            attention_masks.append(encoded_input['attention_mask'])\n",
    "            label_ids.append(label_dict[label])\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=np.int32).squeeze()\n",
    "    token_type_ids = np.array(token_type_ids, dtype=np.int32).squeeze()\n",
    "    attention_masks = np.array(attention_masks, dtype=np.int32).squeeze()\n",
    "    start_positions = np.array(start_positions, dtype=np.int32).squeeze()\n",
    "    end_positions = np.array(end_positions, dtype=np.int32).squeeze()\n",
    "    label_ids = np.array(label_ids, dtype=np.int32).squeeze()\n",
    "\n",
    "    print(\"First text:\\n\", dataset['text'].iloc[0])\n",
    "    print(\"First location:\", dataset['location'].iloc[0])\n",
    "    print(\"First acronym:\", dataset['text'].iloc[0].split()[dataset['location'].iloc[0]])\n",
    "    print(\"First expansion:\", dataset['label'].iloc[0])\n",
    "    print(\"First text decoded:\\n\", tokenizer.decode(input_ids[0]))\n",
    "    print(\"Confirm adjusted location accuracy: \\n\",\n",
    "          tokenizer.decode(input_ids[0][start_positions[0]:end_positions[0]]))\n",
    "    print(\"Confirm label:\", reverse_label_dict[label_ids[0]])\n",
    "\n",
    "    return input_ids, token_type_ids, attention_masks, start_positions, end_positions, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train-------------------------------------------------------------\")\n",
    "train_input_ids, train_token_type_ids, train_attention_masks, train_start_positions, train_end_positions, train_labels = tokenize(train_df)\n",
    "print(\"Val---------------------------------------------------------------\")\n",
    "val_input_ids, val_token_type_ids, val_attention_masks, val_start_positions, val_end_positions, val_labels = tokenize(val_df)\n",
    "print(\"Test---------------------------------------------------------------\")\n",
    "test_input_ids, test_token_type_ids, test_attention_masks, test_start_positions, test_end_positions, test_labels = tokenize(test_df)\n",
    "\n",
    "np.save('../tokenized_medal_inputs//train_input_ids.npy', train_input_ids)\n",
    "np.save('../tokenized_medal_inputs//train_token_type_ids.npy', train_token_type_ids)\n",
    "np.save('../tokenized_medal_inputs//train_attention_masks.npy', train_attention_masks)\n",
    "np.save('../tokenized_medal_inputs//train_start_positions.npy', train_start_positions)\n",
    "np.save('../tokenized_medal_inputs//train_end_positions.npy', train_end_positions)\n",
    "np.save('../tokenized_medal_inputs//train_labels.npy', train_labels)\n",
    "np.save('../tokenized_medal_inputs//val_input_ids.npy', val_input_ids)\n",
    "np.save('../tokenized_medal_inputs//val_token_type_ids.npy', val_token_type_ids)\n",
    "np.save('../tokenized_medal_inputs//val_attention_masks.npy', val_attention_masks)\n",
    "np.save('../tokenized_medal_inputs//val_start_positions.npy', val_start_positions)\n",
    "np.save('../tokenized_medal_inputs//val_end_positions.npy', val_end_positions)\n",
    "np.save('../tokenized_medal_inputs//val_labels.npy', val_labels)\n",
    "np.save('../tokenized_medal_inputs//test_input_ids.npy', test_input_ids)\n",
    "np.save('../tokenized_medal_inputs//test_token_type_ids.npy', test_token_type_ids)\n",
    "np.save('../tokenized_medal_inputs//test_attention_masks.npy', test_attention_masks)\n",
    "np.save('../tokenized_medal_inputs//test_start_positions.npy', test_start_positions)\n",
    "np.save('../tokenized_medal_inputs//test_end_positions.npy', test_end_positions)\n",
    "np.save('../tokenized_medal_inputs//test_labels.npy', test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
