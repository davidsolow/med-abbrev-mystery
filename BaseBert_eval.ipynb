{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 19:27:05.407944: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-27 19:27:05.423504: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-27 19:27:05.423530: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-27 19:27:05.434272: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-27 19:27:06.102627: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/davidsolow/.pyenv/versions/3.11.1/envs/med-abbrev-mystery/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 19:27:08.242918: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-27 19:27:08.250190: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-27 19:27:08.254470: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "#Import statements\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import gc\n",
    "\n",
    "max_length = 200\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'bert-base-cased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_checkpoint)\n",
    "offset = bert_tokenizer.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['physical therapy', 'respiratory insufficiency', 'footpad',\n",
       "       'left anterior descending coronary', 'myoinositol', 't cells',\n",
       "       'dermal papilla', 'peak plasma concentrations', 'kinasedead', 'cbl',\n",
       "       ...\n",
       "       'pressurized liquid extraction', 'leschnyhan', 'raynauds disease',\n",
       "       'biliopancreatic diversion', 'enteroglucagon', 'gastric fistula',\n",
       "       'insideout vesicles', 'transcranial doppler ultrasound',\n",
       "       'prothrombin times', 'highfrequency oscillatory ventilation'],\n",
       "      dtype='object', name='label', length=6500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/medal_smaller_train.csv')\n",
    "test_df = pd.read_csv('data/medal_smaller_validation.csv')\n",
    "\n",
    "samples = train_df.label.value_counts().sort_values(ascending=False)[:6500].index #6500\n",
    "samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict Item 1: ('abasic', 28996)\n",
      "Reverse Dict Item 1: (28996, 'abasic')\n",
      "Test dataset length: 568314\n",
      "Number of classes: 6500\n"
     ]
    }
   ],
   "source": [
    "test_df = test_df.loc[test_df['label'].isin(samples)]\n",
    "\n",
    "with open('dictionary.json', 'r') as f:\n",
    "    label_dict = json.load(f)\n",
    "    label_dict = {key: int(value) for key, value in label_dict.items()}\n",
    "\n",
    "with open('reverse_dictionary.json', 'r') as f:\n",
    "    reverse_label_dict = json.load(f)\n",
    "    reverse_label_dict = {int(key): value for key, value in reverse_label_dict.items()}\n",
    "\n",
    "with open('reverse_dictionary.json', 'r') as f:\n",
    "    reverse_label_dict = json.load(f)\n",
    "    reverse_label_dict = {int(key): value for key, value in reverse_label_dict.items()}\n",
    "\n",
    "with open('label_names.json', 'r') as f:\n",
    "    label_names = json.load(f)\n",
    "\n",
    "print(f\"Dict Item 1: {list(label_dict.items())[0]}\")\n",
    "print(f\"Reverse Dict Item 1: {list(reverse_label_dict.items())[0]}\")\n",
    "\n",
    "print(\"Test dataset length:\", len(test_df))\n",
    "print(\"Number of classes:\", len(label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"Test---------------------------------------------------------------\")\n",
    "# test_input_ids, test_token_type_ids, test_attention_masks, test_start_positions, test_end_positions, test_labels = tokenize(test_df)\n",
    "\n",
    "# np.save('inputs/test_input_ids.npy', test_input_ids)\n",
    "# np.save('inputs/test_token_type_ids.npy', test_token_type_ids)\n",
    "# np.save('inputs/test_attention_masks.npy', test_attention_masks)\n",
    "# np.save('inputs/test_start_positions.npy', test_start_positions)\n",
    "# np.save('inputs/test_end_positions.npy', test_end_positions)\n",
    "# np.save('inputs/test_labels.npy', test_labels)\n",
    "\n",
    "test_input_ids = np.load('inputs/test_input_ids.npy')\n",
    "test_token_type_ids = np.load('inputs/test_token_type_ids.npy')\n",
    "test_attention_masks = np.load('inputs/test_attention_masks.npy')\n",
    "test_start_positions = np.load('inputs/test_start_positions.npy')\n",
    "test_end_positions = np.load('inputs/test_end_positions.npy')\n",
    "test_labels = np.load('inputs/test_labels.npy')\n",
    "\n",
    "samples = np.random.choice(test_input_ids.shape[0], 1000)\n",
    "\n",
    "test_input_ids = test_input_ids[samples]\n",
    "test_token_type_ids = test_token_type_ids[samples]\n",
    "test_attention_masks = test_attention_masks[samples]\n",
    "test_start_positions = test_start_positions[samples]\n",
    "test_end_positions = test_end_positions[samples]\n",
    "test_labels = test_labels[samples]\n",
    "\n",
    "del train_df\n",
    "del test_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractAbbreviationHiddenStates(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom layer that extracts abbreviation embeddings from BERT\n",
    "    hidden layer state and position         padded_slices = padded_array[:, :inputs\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        last_hidden_state, start_abbrev_token_positions, end_abbrev_token_positions = inputs\n",
    "\n",
    "        batch_size = tf.shape(last_hidden_state)[0]\n",
    "        max_length = tf.shape(last_hidden_state)[1]\n",
    "\n",
    "        mask = tf.range(max_length)\n",
    "        mask = tf.tile(mask[tf.newaxis, :], [batch_size, 1])\n",
    "        mask = tf.logical_and(mask >= start_abbrev_token_positions, mask < end_abbrev_token_positions)\n",
    "        span_hidden_state = tf.where(tf.expand_dims(mask, -1), last_hidden_state, tf.zeros_like(last_hidden_state))\n",
    "\n",
    "        return span_hidden_state\n",
    "\n",
    "def create_bert_multiclass_model(checkpoint = model_checkpoint,\n",
    "                                 num_classes = len(label_names),\n",
    "                                 learning_rate=0.00005):\n",
    "    \"\"\"\n",
    "    Build a simple classification model with BERT. Use the pooled abbreviation\n",
    "    token embeddings for classification purposes.\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask_layer')\n",
    "    start_abbrev_token_positions = tf.keras.layers.Input(shape=(1,), dtype=tf.int32, name='start_abbreviation_token_positions_layer')\n",
    "    end_abbrev_token_positions = tf.keras.layers.Input(shape=(1,), dtype=tf.int32, name='end_abbreviation_token_positions_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                   'token_type_ids': token_type_ids,\n",
    "                   'attention_mask': attention_mask}\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(checkpoint, from_pt=True)\n",
    "    bert_model.trainable = True\n",
    "\n",
    "    bert_out = bert_model(bert_inputs)\n",
    "\n",
    "    last_hidden_state = bert_out.last_hidden_state\n",
    "\n",
    "    span_hidden_states = ExtractAbbreviationHiddenStates()([last_hidden_state, start_abbrev_token_positions, end_abbrev_token_positions])\n",
    "\n",
    "    pooled_output = tf.reduce_mean(span_hidden_states, axis=1)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(num_classes, activation='softmax', name='classification_layer')(pooled_output)\n",
    "\n",
    "    classification_model = tf.keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask, start_abbrev_token_positions, end_abbrev_token_positions],\n",
    "        outputs=[classification],\n",
    "    )\n",
    "\n",
    "    def custom_loss(y_true, y_pred, offset=offset):\n",
    "        y_true = y_true - offset\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
    "    \n",
    "    def custom_accuracy(y_true, y_pred):\n",
    "        offset = bert_tokenizer.vocab_size\n",
    "        y_true_adjusted = y_true - offset\n",
    "        return tf.keras.metrics.sparse_categorical_accuracy(y_true_adjusted, y_pred)\n",
    "    \n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                 loss=custom_loss,\n",
    "                                 metrics=[custom_accuracy])\n",
    "\n",
    "    return classification_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 19:27:30.656336: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-27 19:27:30.665213: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-27 19:27:30.674036: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-27 19:27:30.818819: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-27 19:27:30.820439: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-27 19:27:30.822115: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-27 19:27:30.824030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9512 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:08:00.0, compute capability: 8.6\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " attention_mask_layer (Inpu  [(None, 200)]                0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_ids_layer (InputLaye  [(None, 200)]                0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " token_type_ids_layer (Inpu  [(None, 200)]                0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1083102   ['attention_mask_layer[0][0]',\n",
      " )                           ngAndCrossAttentions(last_   72         'input_ids_layer[0][0]',     \n",
      "                             hidden_state=(None, 200, 7              'token_type_ids_layer[0][0]']\n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " start_abbreviation_token_p  [(None, 1)]                  0         []                            \n",
      " ositions_layer (InputLayer                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " end_abbreviation_token_pos  [(None, 1)]                  0         []                            \n",
      " itions_layer (InputLayer)                                                                        \n",
      "                                                                                                  \n",
      " extract_abbreviation_hidde  (None, 200, 768)             0         ['tf_bert_model[0][0]',       \n",
      " n_states (ExtractAbbreviat                                          'start_abbreviation_token_pos\n",
      " ionHiddenStates)                                                   itions_layer[0][0]',          \n",
      "                                                                     'end_abbreviation_token_posit\n",
      "                                                                    ions_layer[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpL  (None, 768)                  0         ['extract_abbreviation_hidden_\n",
      " ambda)                                                             states[0][0]']                \n",
      "                                                                                                  \n",
      " classification_layer (Dens  (None, 6500)                 4998500   ['tf.math.reduce_mean[0][0]'] \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 113308772 (432.24 MB)\n",
      "Trainable params: 113308772 (432.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_bert_multiclass_model()\n",
    "model.load_weights('models/20240725_base_bert_ft_weights.h5')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 11s 108ms/step\n"
     ]
    }
   ],
   "source": [
    "#model = tf.keras.models.load_model('models/20240725_base_bert_ft.keras')\n",
    "\n",
    "test_inputs = [\n",
    "    test_input_ids,\n",
    "    test_token_type_ids,\n",
    "    test_attention_masks,\n",
    "    test_start_positions,\n",
    "    test_end_positions,\n",
    "]\n",
    "\n",
    "predictions = model.predict(test_inputs)\n",
    "y_pred = np.argmax(predictions, axis=-1)\n",
    "y_true = test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 484\n",
      "First X Test:\n",
      " [CLS] LFS is associated with germline tp mutations and carriers have a high lifetime risk of cancer the most common being sarcoma breast cancer BT ACC and leukemia germline tp mutation carriers are increasingly being identified as more genomic sequencing is performed in both clinical and research settings there is a pressing clinical need for effective CA risk management approaches in this group [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "29143\n",
      "First Y True:\n",
      " adrenocortical carcinoma\n",
      "First Y Pred:\n",
      " adrenocortical carcinoma\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.randint(0, test_labels.shape[0])\n",
    "print(\"Sample:\", sample)\n",
    "print(\"First X Test:\\n\", bert_tokenizer.convert_tokens_to_string(bert_tokenizer.convert_ids_to_tokens(test_inputs[0][sample])))\n",
    "print(y_true[sample])\n",
    "print(\"First Y True:\\n\", reverse_label_dict[y_true[sample]])\n",
    "print(\"First Y Pred:\\n\", label_names[y_pred[sample]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med-abbrev-mystery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
