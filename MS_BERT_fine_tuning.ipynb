{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKCfIJUCJfzKKFoRb31+F7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidsolow/med-abbrev-mystery/blob/kiara/MS_BERT_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "0N6jJy5pd1BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "x84JRLG4--Sa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "OVChuaxCOtWU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.37.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_QWllL5RXQO",
        "outputId": "0976d024-104e-4ca3-d9a5-afbe419be50e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.37.2 in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0HsUZ3PNdqwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa4bbf2a-f288-4f9d-b62d-146d68d87460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"drive/MyDrive/266Project/train-3.csv\")\n",
        "#test = pd.read_csv(\"drive/MyDrive/266Project/test.csv\")\n",
        "validation = pd.read_csv(\"drive/MyDrive/266Project/validation.csv\")"
      ],
      "metadata": {
        "id": "t5wvp1OKeRkR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.sample(frac=0.005)\n",
        "validation = validation.sample(frac=0.01)\n",
        "print(len(train))\n",
        "print(len(validation))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJKG_iJt-c3K",
        "outputId": "3acee4b6-4314-4899-9fd7-ff68eedd8d38"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "access_token = \"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\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"NLP4H/ms_bert\", token = access_token)\n",
        "model = TFBertModel.from_pretrained(\"NLP4H/ms_bert\", token = access_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vmMcomM-kXd",
        "outputId": "23a488e9-c20b-4615-d0ae-b308af6caff7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning location and label columns\n",
        "def clean_location(location):\n",
        "  \"\"\"Takes a number in brackets as input and reterns the number as an int\"\"\"\n",
        "  return int(str(location).strip(\"[]\"))\n",
        "\n",
        "def clean_label(label):\n",
        "  \"\"\"Takes a label in brackets and quotes as input and reterns the label as a string\"\"\"\n",
        "  return label.strip(\"[]'\")\n",
        "\n",
        "for dataset in [train, validation]:\n",
        "  dataset['location'] = dataset['location'].apply(clean_location)\n",
        "  dataset['label'] = dataset['label'].apply(clean_label)"
      ],
      "metadata": {
        "id": "AyNnjVdNedQt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting labels to integers\n",
        "def make_label_map(labels):\n",
        "  label_map = {}\n",
        "  for i in range(len(labels.unique())):\n",
        "    label_map[labels.unique()[i]] = i\n",
        "  return label_map\n",
        "\n",
        "label_map = make_label_map(train['label'])\n",
        "valid_labels = label_map.keys()\n",
        "validation = validation[validation['label'].isin(valid_labels)]\n",
        "for dataset in [train, validation]:\n",
        "   dataset.loc[:, 'label'] = dataset['label'].map(label_map)\n",
        "\n",
        "num_classes = len(label_map)"
      ],
      "metadata": {
        "id": "eJFl_8Cneg5q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAGL0XgZBXqm",
        "outputId": "b1c5587a-8008-4256-a308-5a46768e9247"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filtering by text length and location\n",
        "max_length = 200\n",
        "max_location = max_length - 3 # minus [CLS] and [SEP] tokens added and index offset\n",
        "\n",
        "def add_abbreviation_col(dataset):\n",
        "    \"\"\"Adds an abbreviation column to the dataset from the specified location in the text\"\"\"\n",
        "    dataset['abbreviation'] = dataset.apply(lambda row: row['text'].split()[row['location']], axis=1)\n",
        "    return dataset\n",
        "\n",
        "def clean_dataset(dataset):\n",
        "    dataset = dataset.loc[dataset['location'] <= max_location].copy()\n",
        "    return dataset\n",
        "\n",
        "for dataset in [train, validation]:\n",
        "  dataset['abbreviation'] = dataset.apply(lambda row: row['text'].split()[row['location']], axis=1)\n",
        "  clean_dataset(dataset)"
      ],
      "metadata": {
        "id": "OGNVUAD6YnHO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "wnIWv82bG8cv",
        "outputId": "7cc7404b-34ab-4904-ff19-9f15dceda1de"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Unnamed: 0  abstract_id  \\\n",
              "223160       223160      1004864   \n",
              "2846266     2846266      8270621   \n",
              "2908905     2908905      3319055   \n",
              "1248734     1248734      8300813   \n",
              "192559       192559      2482370   \n",
              "\n",
              "                                                      text  location label  \\\n",
              "223160   cardiovascular responsiveness to sympathoadren...       169     0   \n",
              "2846266  various strategies have been studied to reduce...        10     1   \n",
              "2908905  a dayold boy presented to our emergency depart...       110     2   \n",
              "1248734  tularemia is caused by two subspecies of franc...        57     3   \n",
              "192559   we have examined the responsiveness of the ver...        50     4   \n",
              "\n",
              "        abbreviation  \n",
              "223160           VET  \n",
              "2846266          ROC  \n",
              "2908905          IPA  \n",
              "1248734          SSH  \n",
              "192559            Ra  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90cff9e0-a788-486e-ab1d-7c5e533e1c89\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>abstract_id</th>\n",
              "      <th>text</th>\n",
              "      <th>location</th>\n",
              "      <th>label</th>\n",
              "      <th>abbreviation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>223160</th>\n",
              "      <td>223160</td>\n",
              "      <td>1004864</td>\n",
              "      <td>cardiovascular responsiveness to sympathoadren...</td>\n",
              "      <td>169</td>\n",
              "      <td>0</td>\n",
              "      <td>VET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2846266</th>\n",
              "      <td>2846266</td>\n",
              "      <td>8270621</td>\n",
              "      <td>various strategies have been studied to reduce...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>ROC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2908905</th>\n",
              "      <td>2908905</td>\n",
              "      <td>3319055</td>\n",
              "      <td>a dayold boy presented to our emergency depart...</td>\n",
              "      <td>110</td>\n",
              "      <td>2</td>\n",
              "      <td>IPA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1248734</th>\n",
              "      <td>1248734</td>\n",
              "      <td>8300813</td>\n",
              "      <td>tularemia is caused by two subspecies of franc...</td>\n",
              "      <td>57</td>\n",
              "      <td>3</td>\n",
              "      <td>SSH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192559</th>\n",
              "      <td>192559</td>\n",
              "      <td>2482370</td>\n",
              "      <td>we have examined the responsiveness of the ver...</td>\n",
              "      <td>50</td>\n",
              "      <td>4</td>\n",
              "      <td>Ra</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90cff9e0-a788-486e-ab1d-7c5e533e1c89')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-90cff9e0-a788-486e-ab1d-7c5e533e1c89 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-90cff9e0-a788-486e-ab1d-7c5e533e1c89');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-17ec86e4-d68e-4db9-b040-1a0f0ac18230\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-17ec86e4-d68e-4db9-b040-1a0f0ac18230')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-17ec86e4-d68e-4db9-b040-1a0f0ac18230 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_abbrev_token_positions(text, abbrev):\n",
        "    \"\"\"\n",
        "    Takes text and abbreviation and finds the start and end index of the\n",
        "    tokenized representation of that abbreviation in the text\n",
        "    \"\"\"\n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "    tokenized_abbrev = tokenizer.tokenize(abbrev)\n",
        "    token_ids_text = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    token_ids_abbrev = tokenizer.convert_tokens_to_ids(tokenized_abbrev)\n",
        "    start = -1\n",
        "    for i in range(len(token_ids_text) - len(token_ids_abbrev) + 1):\n",
        "        if token_ids_text[i:i+len(token_ids_abbrev)] == token_ids_abbrev:\n",
        "            start = i\n",
        "            break\n",
        "\n",
        "    if start == -1:\n",
        "        raise ValueError(f\"Abbreviation '{abbrev}' not found in text '{text}'\")\n",
        "\n",
        "    end = start + len(token_ids_abbrev) - 1\n",
        "    return start, end\n",
        "\n",
        "\n",
        "def extract_abbrev_positions_from_dataset(dataset):\n",
        "    \"\"\"Extracts all the start and end position of each abbreviation in a dataset\"\"\"\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i, row in dataset.iterrows():\n",
        "        start, end = get_abbrev_token_positions(row['text'], row['abbreviation'])\n",
        "        start_positions.append(start + 1)  # add 1 to account for CLS token at start\n",
        "        end_positions.append(end + 1)\n",
        "\n",
        "    return start_positions, end_positions\n",
        "\n",
        "\n",
        "train_start_positions, train_end_positions = extract_abbrev_positions_from_dataset(train)\n",
        "valid_start_positions, valid_end_positions = extract_abbrev_positions_from_dataset(validation)\n",
        "#test_start_positions, test_end_positions = extract_abbrev_positions_from_dataset(test_subset)"
      ],
      "metadata": {
        "id": "QEZuIZM2EFje"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 200\n",
        "train_list = train.text.tolist()\n",
        "validation_list = validation.text.tolist()\n",
        "\n",
        "train_tokenized = tokenizer(train_list,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "\n",
        "train_inputs = [train_tokenized.input_ids,\n",
        "                train_tokenized.token_type_ids,\n",
        "                train_tokenized.attention_mask]\n",
        "\n",
        "train_labels = np.array(train.label)\n",
        "\n",
        "validation_tokenized=tokenizer(validation_list,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "\n",
        "validation_inputs = [validation_tokenized.input_ids,\n",
        "                     validation_tokenized.token_type_ids,\n",
        "                     validation_tokenized.attention_mask]\n",
        "\n",
        "validation_labels = np.array(validation.label)\n",
        "train_locations = tf.convert_to_tensor(np.array(train.location), dtype=tf.int32)\n",
        "validation_locations = tf.convert_to_tensor(np.array(validation.location), dtype=tf.int32)"
      ],
      "metadata": {
        "id": "c2cXNpzG9VEF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 42"
      ],
      "metadata": {
        "id": "m-0BY1fZI5Yu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtractAbbreviationHiddenStates(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom layer that extracts abbreviation embeddings from BERT\n",
        "    hidden layer state and position inputs\n",
        "    \"\"\"\n",
        "    def call(self, inputs):\n",
        "        last_hidden_state, start_abbrev_token_positions, end_abbrev_token_positions = inputs\n",
        "        batch_size = tf.shape(last_hidden_state)[0]\n",
        "\n",
        "        max_length = tf.shape(last_hidden_state)[1]\n",
        "        hidden_size = tf.shape(last_hidden_state)[2]\n",
        "\n",
        "        span_hidden_states = tf.TensorArray(tf.float32, size=batch_size)\n",
        "\n",
        "        for i in tf.range(batch_size):\n",
        "            start_pos = start_abbrev_token_positions[i, 0]\n",
        "            end_pos = end_abbrev_token_positions[i, 0]\n",
        "\n",
        "            start_pos = tf.clip_by_value(start_pos, 0, max_length - 1)\n",
        "            end_pos = tf.clip_by_value(end_pos, 0, max_length - 1)\n",
        "\n",
        "            span_hidden_state = last_hidden_state[i, start_pos:end_pos + 1, :]\n",
        "            span_length = end_pos - start_pos + 1\n",
        "\n",
        "            # pad to the maximum length\n",
        "            padded_span_hidden_state = tf.pad(span_hidden_state, [[0, max_length - span_length], [0, 0]])\n",
        "            span_hidden_states = span_hidden_states.write(i, padded_span_hidden_state)\n",
        "\n",
        "        return span_hidden_states.stack()\n",
        "\n",
        "def create_bert_multiclass_model(checkpoint=\"NLP4H/ms_bert\", num_classes=10, learning_rate=0.00005):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with BERT. Use the Pooler Output for classification purposes.\n",
        "    \"\"\"\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    input_ids = tf.keras.layers.Input(shape=(200,), dtype=tf.int32, name='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(200,), dtype=tf.int32, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(200,), dtype=tf.int32, name='attention_mask_layer')\n",
        "    start_abbrev_token_positions = tf.keras.layers.Input(shape=(1,), dtype=tf.int32, name='start_abbreviation_token_positions_layer')\n",
        "    end_abbrev_token_positions = tf.keras.layers.Input(shape=(1,), dtype=tf.int32, name='end_abbreviation_token_positions_layer')\n",
        "\n",
        "    bert_model = TFBertModel.from_pretrained(checkpoint)\n",
        "    bert_inputs = [input_ids, attention_mask, token_type_ids]\n",
        "\n",
        "    bert_out = bert_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "    last_hidden_state = bert_out.last_hidden_state\n",
        "\n",
        "    span_hidden_states = ExtractAbbreviationHiddenStates()([last_hidden_state, start_abbrev_token_positions, end_abbrev_token_positions])\n",
        "    pooled_output = tf.reduce_mean(span_hidden_states, axis=1)\n",
        "\n",
        "    classification = tf.keras.layers.Dense(num_classes, activation='softmax', name='classification_layer')(pooled_output)\n",
        "\n",
        "    classification_model = tf.keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask, start_abbrev_token_positions, end_abbrev_token_positions],\n",
        "        outputs=[classification],\n",
        "    )\n",
        "\n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                                 metrics='accuracy')\n",
        "\n",
        "    return classification_model\n"
      ],
      "metadata": {
        "id": "E0j7M0TIEVXw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abbreviation_model = create_bert_multiclass_model()\n",
        "abbreviation_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZoQdRAgIx0h",
        "outputId": "34f3e9d4-d0ce-419f-983e-7c0cd5947104"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_ids_layer (InputLaye  [(None, 200)]                0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " attention_mask_layer (Inpu  [(None, 200)]                0         []                            \n",
            " tLayer)                                                                                          \n",
            "                                                                                                  \n",
            " token_type_ids_layer (Inpu  [(None, 200)]                0         []                            \n",
            " tLayer)                                                                                          \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1094822   ['input_ids_layer[0][0]',     \n",
            " )                           ngAndCrossAttentions(last_   40         'attention_mask_layer[0][0]',\n",
            "                             hidden_state=(None, 200, 7              'token_type_ids_layer[0][0]']\n",
            "                             68),                                                                 \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " start_abbreviation_token_p  [(None, 1)]                  0         []                            \n",
            " ositions_layer (InputLayer                                                                       \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " end_abbreviation_token_pos  [(None, 1)]                  0         []                            \n",
            " itions_layer (InputLayer)                                                                        \n",
            "                                                                                                  \n",
            " extract_abbreviation_hidde  (None, None, 768)            0         ['tf_bert_model[0][0]',       \n",
            " n_states (ExtractAbbreviat                                          'start_abbreviation_token_pos\n",
            " ionHiddenStates)                                                   itions_layer[0][0]',          \n",
            "                                                                     'end_abbreviation_token_posit\n",
            "                                                                    ions_layer[0][0]']            \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpL  (None, 768)                  0         ['extract_abbreviation_hidden_\n",
            " ambda)                                                             states[0][0]']                \n",
            "                                                                                                  \n",
            " classification_layer (Dens  (None, 10)                   7690      ['tf.math.reduce_mean[0][0]'] \n",
            " e)                                                                                               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109489930 (417.67 MB)\n",
            "Trainable params: 109489930 (417.67 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs = [\n",
        "    np.array(train_tokenized.input_ids, dtype=np.int32),\n",
        "    np.array(train_tokenized.token_type_ids, dtype=np.int32),\n",
        "    np.array(train_tokenized.attention_mask, dtype=np.int32),\n",
        "    np.array(train_start_positions, dtype=np.int32),\n",
        "    np.array(train_end_positions, dtype=np.int32),\n",
        "]\n",
        "\n",
        "valid_inputs = [\n",
        "    np.array(validation_tokenized.input_ids, dtype=np.int32),\n",
        "    np.array(validation_tokenized.token_type_ids, dtype=np.int32),\n",
        "    np.array(validation_tokenized.attention_mask, dtype=np.int32),\n",
        "    np.array(valid_start_positions, dtype=np.int32),\n",
        "    np.array(valid_end_positions, dtype=np.int32),\n",
        "]\n",
        "\n",
        "train_labels = np.array(train_labels, dtype=np.int32)\n",
        "validation_labels = np.array(validation_labels, dtype=np.int32)\n",
        "\n",
        "history = abbreviation_model.fit(\n",
        "    train_inputs,\n",
        "    train_labels,\n",
        "    validation_data=(valid_inputs, validation_labels),\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    epochs=1,\n",
        ")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onZdjIURAHUf",
        "outputId": "edb52014-3f43-4ad9-c9a3-b8593b5c45b6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938/938 [==============================] - 672s 676ms/step - loss: nan - accuracy: 2.0000e-04 - val_loss: nan - val_accuracy: 1.5873e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cXyaNNPUIR_n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}